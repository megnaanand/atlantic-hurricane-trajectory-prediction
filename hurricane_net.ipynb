{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoYl6itUaG8M"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKZpPn-6aSjm"
      },
      "outputs": [],
      "source": [
        "%cd gdrive/MyDrive/atlantic-hurricane-trajectory-prediction/docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wksjVxR-cZph"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wU6pXh5ycf8N"
      },
      "outputs": [],
      "source": [
        "%autosave 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elmYIsXechGh"
      },
      "outputs": [],
      "source": [
        "# Import various libraries throughout the software\n",
        "from pprint import pprint\n",
        "from datetime import timedelta\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from geopy.distance import great_circle as vc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math as Math\n",
        "import datetime\n",
        "import dateutil\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaLbuPxDdnVL"
      },
      "outputs": [],
      "source": [
        "# data cleaning/processing: (from hurricane-net, hammad)\n",
        "db = []\n",
        "with open('data/hurdat2-1851-2022-050423.txt') as raw: \n",
        "    for line in raw: \n",
        "        line = line.replace(' ', '').split(',')\n",
        "    \n",
        "        # Identify atlantic storm, first 2 letters should be AL\n",
        "        if (line[0][:2] == 'AL') :\n",
        "            storm_id = line[0]\n",
        "            storm_name = line[1]\n",
        "            storm_entries = line[2]\n",
        "\n",
        "            # Iterate and read through best track entries\n",
        "            for i in range(int(storm_entries)) :\n",
        "                entry = raw.readline().replace(' ', '').split(',')\n",
        "                # Filter -999 placeholder for missing central pressure\n",
        "                entry = [None if x == \"-999\" else x for x in entry]\n",
        "                # Construct date and time based on first two columns\n",
        "                timestamp = datetime.datetime(int(entry[0][:4]), int(entry[0][4:6]), int(entry[0][6:8]), int(entry[1][:2]), int(entry[1][3:]))\n",
        "                # Add entry into our current database\n",
        "                db.append([storm_id, storm_name, timestamp] + entry[2:-1])\n",
        "        else :\n",
        "            print(\"Error, unidentified storm \".join(str(line[0])))\n",
        "\n",
        "# Return DataFrame\n",
        "dataset = pd.DataFrame(db, columns = ['storm_id', 'storm_name', 'entry_time', 'entry_id', 'entry_status', 'lat', 'long','max_wind', 'min_pressure', '34kt_ne', '34kt_se', '34kt_sw', '34kt_nw', '50kt_ne', '50kt_se', '50kt_sw', '50kt_nw', '64kt_ne', '64kt_se', '64kt_sw', '64kt_nw'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVqCzNzDmg0R"
      },
      "outputs": [],
      "source": [
        "models = dict()\n",
        "class model :\n",
        "  '''\n",
        "  PURPOSE: To create a class for each model included in the forecast error database\n",
        "  METHOD: Provide an API\n",
        "  OUTOUT: A class with a DataFrame and associated operations\n",
        "  '''\n",
        "  name = None\n",
        "  # Dictionary key: STMID\n",
        "  storm = dict()\n",
        "  def __init__(self, model_name) :\n",
        "    self.name = model_name\n",
        "    return\n",
        "\n",
        "with open('errors/1970-present_OFCL_v_BCD5_ind_ATL_TI_errors_noTDs.txt') as raw :\n",
        "    lines = raw.readlines()\n",
        "    \n",
        "    # Get model names and declare model objects\n",
        "    line = lines[1].split()\n",
        "    model_names = line[2:]\n",
        "    for model_name in model_names :\n",
        "        models[model_name] = model(model_name)\n",
        "    \n",
        "    # Data starts at line 9 \n",
        "    for line in lines[9:] :\n",
        "        line = line.split()\n",
        "        # Identify atlantic storm date, storm id, associated sample sizes, latitude and longitude, and windspeed\n",
        "        timestamp = datetime.datetime.strptime(line[0], \"%d-%m-%Y/%H:%M:%S\")\n",
        "        storm_id = line[1]\n",
        "        sample_sizes = {\"F012\": float(line[2]), \"F024\": float(line[3]),\"F036\": float(line[4]), \"F048\": float(line[5]), \"F072\": float(line[6]), \"F096\": float(line[7]), \"F120\": float(line[8]), \"F144\": float(line[9]), \"F168\": float(line[10])} \n",
        "        latitude = float(line[11])\n",
        "        longitude = float(line[12])\n",
        "        wind_speed = float(line[13])\n",
        "    \n",
        "                \n",
        "        # Iterate through model forecast track and intensity errors \n",
        "        for i in range(len(model_names)) :\n",
        "            intensity_forecast = dict(list(zip([timestamp, timestamp + timedelta(hours = 12), timestamp + timedelta(hours = 24), timestamp + timedelta(hours = 36), timestamp + timedelta(hours = 48), timestamp + timedelta(hours = 72), timestamp + timedelta(hours = 96), timestamp + timedelta(hours = 120), timestamp + timedelta(hours = 144), timestamp + timedelta(hours = 168)], [None if x == \"-9999.0\" else float(x) for x in line[14 + (20 * i) : 24 + (20 * i)]])))\n",
        "            track_forecast = dict(list(zip([timestamp, timestamp + timedelta(hours = 12), timestamp + timedelta(hours = 24), timestamp + timedelta(hours = 36), timestamp + timedelta(hours = 48), timestamp + timedelta(hours = 72), timestamp + timedelta(hours = 96), timestamp + timedelta(hours = 120), timestamp + timedelta(hours = 144), timestamp + timedelta(hours = 168)], [None if x == \"-9999.0\" else float(x) for x in line[24 + (20 * i) : 34 + (20 * i)]])))\n",
        "        \n",
        "        # Add forecast to model and storm, initialize if storm id does not exist\n",
        "        if storm_id not in models[model_names[i]].storm.keys() :\n",
        "            models[model_names[i]].storm[storm_id] = dict()\n",
        "\n",
        "        models[model_names[i]].storm[storm_id].update({\n",
        "            timestamp : {\n",
        "            \"sample_sizes\" : sample_sizes,\n",
        "            \"lat\" : latitude,\n",
        "            \"long\" : longitude,\n",
        "            \"wind_speed\" : wind_speed,\n",
        "            \"intensity_forecast\" : intensity_forecast,\n",
        "            \"track_forecast\" : track_forecast,\n",
        "            }\n",
        "        })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hG4AXCzkl7Sw"
      },
      "outputs": [],
      "source": [
        "# Show the first 3 OFCL hurricane model errors for Hurricane Katrina 2005 on 28-08-2005/18:00:00\n",
        "pprint(models['OFCL'].storm['AL122005'][datetime.datetime(2005, 8, 28, 18, 0)], indent = 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jC4jkJSG7-C"
      },
      "outputs": [],
      "source": [
        "# Show the first 5 records from Hurricane Katrina 2005 (AL122005)\n",
        "dataset.query('storm_id == \"AL122005\"').head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1USsREqJHDnG"
      },
      "source": [
        "# Transform Data\n",
        "The following code will tranform the hurricane best path data into objects that can be better manipulated for processing. to match between datasets, we will also create a storm_id dictionary to store storm names matched with ID's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_Db-7PBHHBR"
      },
      "outputs": [],
      "source": [
        "# Create hurricane class\n",
        "class hurricane(object) : \n",
        "    def __init__(self, name, id) :\n",
        "        # Set instance variables\n",
        "        self.name = name\n",
        "        self.id = id\n",
        "        self.entries = dict()\n",
        "        self.models = dict()\n",
        "        \n",
        "        return\n",
        "    # Add hurricane track entry based on standard HURDAT2 format\n",
        "    def add_entry(self, array) :\n",
        "        entry = {\n",
        "            array[0] : { # dateteime of entry\n",
        "                'entry_time' : array[0], \n",
        "                'entry_id' : array[1],\n",
        "                'entry_status' : array[2],\n",
        "                'lat' : float(array[3][:-1]), # Convert to number from format '#.#N'\n",
        "                'long' : float(array[4][:-1]), # Convert to number from format '#.#W'\n",
        "                'max_wind' : float(array[5]),\n",
        "                'min_pressure' : 980 if array[6] is None else float(array[6]), # Early records are -999 or None\n",
        "                'wind_radii' :  array[7:], # Array based on HURDAT2 format\n",
        "                'distance': 0,\n",
        "                'direction': 0\n",
        "            }\n",
        "        }\n",
        "        self.entries.update(entry)\n",
        "        \n",
        "        return\n",
        "    # Add hurricane model errors\n",
        "    def add_model(self, name, model) :\n",
        "        self.models[name] = model\n",
        "        \n",
        "        return\n",
        "\n",
        "\n",
        "    def update_dist_direc(self):\n",
        "      t = pd.DataFrame(self.entries.values())\n",
        "      dst = 0\n",
        "      prev = (0,0)\n",
        "      \n",
        "      # For all latitude and longitude points of hurricane, calculate the angle of travel and distance\n",
        "      for index,p in enumerate(zip(t['lat'], t['long'])):\n",
        "          \n",
        "          if prev == (0,0):\n",
        "              prev = p\n",
        "              continue \n",
        "          # Stores the distance into the DataFrame\n",
        "          list(self.entries.values())[index]['distance'] = vc(prev,p).miles\n",
        "          \n",
        "          dLon = p[1] - prev[1];  \n",
        "          temp = float(p[0]) # p[0] is a str?\n",
        "          y_x = Math.sin(dLon) * Math.cos(temp);\n",
        "          \n",
        "          x_x = Math.cos(p[1]) * Math.sin(temp) - Math.sin(p[1]) * Math.cos(temp) * Math.cos(dLon);\n",
        "          brng = Math.degrees(Math.atan2(y_x, x_x)) \n",
        "          if (brng < 0):\n",
        "              brng+= 360;\n",
        "          \n",
        "          # Stores the angle of travel into the DataFrame\n",
        "          list(self.entries.values())[index]['direction'] = brng\n",
        "          # if self.id == 'AL122005' and index==2:\n",
        "          if self.id == 'AL081994' and index==2:\n",
        "            print(f'p[1]:{p[1]}')\n",
        "            print(f'prev[1]:{prev[1]}')\n",
        "            print(f'dLon:{dLon}')\n",
        "            print(f'temp:{temp}')\n",
        "            print(f'y_x:{y_x}')\n",
        "            print(f'x_x:{x_x}')\n",
        "            print(f'brng:{brng}')\n",
        "          dst += vc(prev,p).miles\n",
        "          prev = p\n",
        "\n",
        "# Storm ID Key for matching between datasets\n",
        "storm_ids = dict()\n",
        "# Parse in hurricanes\n",
        "hurricanes = dict()\n",
        "\n",
        "print(\"Transforming HURDAT2 into objects . . .\")\n",
        "for index, entry in dataset.iterrows() :\n",
        "    print(\"Transforming {}/{} entries from HURDAT2\".format(index + 1, len(dataset)), end = \"\\r\")\n",
        "    # New hurricane\n",
        "    if entry['storm_id'] not in hurricanes :\n",
        "        hurricanes[entry['storm_id']] = hurricane(entry['storm_name'], entry['storm_id'])\n",
        "        storm_ids[entry['storm_id']] = entry['storm_name']\n",
        "    # Add entry to hurricane\n",
        "    hurricanes[entry['storm_id']].add_entry(entry[2:])\n",
        "print(\"\\nDone!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shxGx5Rg-uN_"
      },
      "source": [
        "# Load Data\n",
        "The following will finalize our preliminary data preparation by loading some of the errors into each hurricane object. Note that models start from the year 1970 and any hurricane before that has no previous model data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPbnSHF8-s9O"
      },
      "outputs": [],
      "source": [
        "# Get all available model errors\n",
        "# Load model errors into hurricanes\n",
        "for id in storm_ids :\n",
        "    for model in models :\n",
        "        # Skip if this hurricane does not have the model\n",
        "        if id not in models[model].storm :\n",
        "            continue\n",
        "        hurricanes[id].add_model(model, models[model].storm[id])\n",
        "    hurricanes[id].update_dist_direc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZL5n_PJCTh1"
      },
      "outputs": [],
      "source": [
        "#will test distance and direction of the bulk update vs individual update\n",
        "t=pd.DataFrame(hurricanes['AL081994'].entries.values())\n",
        "t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3EhCEBPDa_j"
      },
      "outputs": [],
      "source": [
        "t['dist']=0\n",
        "t['direc']=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPUaZqElDiHF"
      },
      "outputs": [],
      "source": [
        "#testing for one hurricane\n",
        "prev=(0,0)\n",
        "for index,p in enumerate(zip(t['lat'], t['long'])):\n",
        "  if prev == (0,0):\n",
        "    prev = p\n",
        "    print(f'index:{index},prev:{prev}')\n",
        "    continue \n",
        "  # Stores the distance into the DataFrame\n",
        "  t.at[index,'dist'] = vc(prev,p).miles\n",
        "\n",
        "  dLon = p[1] - prev[1];  \n",
        "  temp = float(p[0]) # p[0] is a str?\n",
        "  y_x = Math.sin(dLon) * Math.cos(temp);\n",
        "\n",
        "  x_x = Math.cos(p[1]) * Math.sin(temp) - Math.sin(p[1]) * Math.cos(temp) * Math.cos(dLon);\n",
        "  brng = Math.degrees(Math.atan2(y_x, x_x)) \n",
        "  if (brng < 0):\n",
        "    brng+= 360;\n",
        "  t.at[index,'direc'] = brng\n",
        "  if index==2:\n",
        "    print(f'p[1]:{p[1]}')\n",
        "    print(f'prev[1]:{prev[1]}')\n",
        "    print(f'dLon:{dLon}')\n",
        "    print(f'temp:{temp}')\n",
        "    print(f'y_x:{y_x}')\n",
        "    print(f'x_x:{x_x}')\n",
        "    print(f'brng:{brng}')\n",
        "  print(index,t.at[index,'direc'])\n",
        "  prev = p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-6f67phDjvJ"
      },
      "outputs": [],
      "source": [
        "t #to check that distance calculation and direction calculation from bulk vs individual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwdLRdvVDrxM"
      },
      "outputs": [],
      "source": [
        "models.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kt3z6MwrD23n"
      },
      "source": [
        "# Feature Engineering & Data Augmentation\n",
        "The following section will extract the relevant features and engineer each data point so that we can fit it into the model. Because the type of inputs are important, the features will be transformed based on the model architecture. This will also include data augmentation methods. The higher level architecture will be a deep learning recurrent neural network with LSTM and time distributed layers.\n",
        "\n",
        "The current statistical baseline model using multivariate regression uses multiple predictors as input. According to Knaff 2013, the following predictors were calculated for their intensity model that were not included in the HURDAT2 database. These features can be calculated from the data loaded into our current object model.\n",
        "1. Date Information\n",
        "2. Zonal Speed Of The Storm (U) (kt)\n",
        "3. Meridional Speed Of The Storm (V) (kt)\n",
        "4. 12-h Change In Intensity (DVMX) (kt)\n",
        "\n",
        "The shape on the input to the LSTM will be in a 3D array with the format [samples, timestamps, features]. We will intitially begin with 1 time step and evaluate more can benefit our model. The output requires a 5 day forecast and observations without track data 5 days in the future will not be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LE9LxW_OD7q8"
      },
      "outputs": [],
      "source": [
        "def feature_extraction(timestep, previous) :\n",
        "    '''\n",
        "    PURPOSE: Calculate the features for a machine learning model within the context of hurricane-net\n",
        "    METHOD: Use the predictors and the calculation methodology defined in Knaff 2013\n",
        "    INPUT:  timestep - current dictionary of features in the hurricane object format\n",
        "            previous - previous timestep dictionary of features in the hurricane object format\n",
        "    OUTPUT: Dictionary of features\n",
        "    \n",
        "    timestep = {\n",
        "      'lat' : float,\n",
        "      'long' : float,\n",
        "      'max-wind' : float,\n",
        "      'entry-time' : datetime\n",
        "    }\n",
        "    '''\n",
        "    features = {\n",
        "        'lat' : timestep['lat'],\n",
        "        'long' : timestep['long'],\n",
        "        'max_wind' : timestep['max_wind'],\n",
        "        'delta_wind' : (timestep['max_wind'] - previous['max_wind']) / # Calculated from track (12h)\n",
        "            ((timestep['entry_time'] - previous['entry_time']).total_seconds() / 43200),\n",
        "        'min_pressure' : timestep['min_pressure'], \n",
        "        'zonal_speed' : (timestep['lat'] - previous['lat'])/ # Calculated from track (per hour)\n",
        "            ((timestep['entry_time'] - previous['entry_time']).total_seconds() / 3600),\n",
        "        'meridonal_speed' : (timestep['long'] - previous['long'])/# Calculated from track (per hour)\n",
        "            ((timestep['entry_time'] - previous['entry_time']).total_seconds() / 3600),\n",
        "        'year' : timestep['entry_time'].year,\n",
        "        'month' : timestep['entry_time'].month,\n",
        "        'day' : timestep['entry_time'].day,\n",
        "        'hour' : timestep['entry_time'].hour,\n",
        "        'delta_pressure': (timestep['min_pressure'] - previous['min_pressure']) /\n",
        "            ((timestep['entry_time'] - previous['entry_time']).total_seconds() / 43200),\n",
        "        'distance': timestep['distance'],\n",
        "        'direction': timestep['direction']\n",
        "    }\n",
        "    return features\n",
        "    \n",
        "def storm_x_y(storm, timesteps = 1, lag = 24) :\n",
        "    '''\n",
        "    PURPOSE: Create independent and dependent samples for a machine learning model based on the timesteps\n",
        "    METHOD: Use the HURDAT2 database and a hurricane object as defined in hurricane-net for feature extraction\n",
        "    INPUT:  storm - hurricane object\n",
        "            timesteps - (default = 1) number of timesteps to calculate\n",
        "            include_none - (default = False) Boolean for including None in test data. Imputing function unavailable.\n",
        "            lag - (default = 24) lag in hours for the dependent variables up to 5 days\n",
        "    OUTPUT: Dictionary with independent (x) and dependent (y) values.\n",
        "    '''\n",
        "    x = []\n",
        "    # Create testing data structure with a dictionary\n",
        "    times = [time * lag for time in range(1, (120 // lag) + 1)] # Begin at lag hours with lag increments up to 120h inclusive\n",
        "    y = dict([(time,[]) for time in times])\n",
        "    \n",
        "    # Sort by entry time\n",
        "    entries = [entry[1] for entry in sorted(storm.entries.items())]\n",
        "    \n",
        "    for index in range(len(entries)) :\n",
        "        if index < timesteps : # Flag for insufficient initial time steps\n",
        "            continue\n",
        "\n",
        "        # If we're not including None values, check to see if there will be any\n",
        "        if None in [storm.entries.get(entries[index]['entry_time'] +\n",
        "                                         datetime.timedelta(hours = future)) for future in times] : break\n",
        "            \n",
        "        # Calculate time steps and their features for independent values\n",
        "        sample = []\n",
        "        for step in range(timesteps) :\n",
        "            # Training sample\n",
        "            timestep = entries[index - step]\n",
        "            previous = entries[index - step - 1]\n",
        "            sample.append([timestep['entry_time']] + [[feature_extraction(timestep, previous)]])\n",
        "        x.append(sample) # Add our constructed sample\n",
        "        \n",
        "        # Calculate time steps and their features for dependent values\n",
        "        for future in times :\n",
        "            timestep = storm.entries.get(entries[index]['entry_time'] + datetime.timedelta(hours = future))\n",
        "            previous = storm.entries.get(entries[index]['entry_time'] + datetime.timedelta(hours = future - lag))\n",
        "            \n",
        "            if timestep and previous: \n",
        "                y[future].append(feature_extraction(timestep, previous))\n",
        "            else :\n",
        "                y[future].append(None)\n",
        "    \n",
        "    # Return output, if there is no output, return None.\n",
        "    if len(x) == 0 :\n",
        "        return None\n",
        "    else:\n",
        "        return {'x': x, 'y': y}\n",
        "def shape(hurricanes, timesteps, remove_missing = True) :\n",
        "    '''\n",
        "    PURPOSE: Shape our data for input into machine learning models\n",
        "    METHOD: Use a numpy array to shape into (samples, timesteps, features)\n",
        "    INPUT:  hurricanes - dictionary of hurricane objects\n",
        "            timesteps - number of timesteps for the shape\n",
        "            remove_missing - boolean indicating whether the algorithm will disregard missing values\n",
        "    OUTPUT: numpy array of shape (samples, timesteps, 11) where 11 is the number of predictors in a hurricane object\n",
        "    '''\n",
        "    x = []\n",
        "    y = []\n",
        "    lag = 24 # lag time in hours\n",
        "    precision = np.float64 # defines the precision of our data type\n",
        "    times = [time * lag for time in range(1, (120 // lag) + 1)] # Begin at lag hours with lag increments up to 120h inclusive\n",
        "    count = 0\n",
        "    for hurricane in hurricanes.values() :\n",
        "        count += 1\n",
        "        result = storm_x_y(hurricane, timesteps, lag)\n",
        "        if result is None :\n",
        "            continue\n",
        "        # Extract only the values from the strom features using our specified precision\n",
        "        hurricane_x = np.array(\n",
        "            [[list(sample[1][0].values()) for sample in x] for x in result['x']],\n",
        "            dtype = precision)\n",
        "        hurricane_y = np.array(\n",
        "            [[list(result['y'][time][index].values()) for time in times] for index in range(len(result['y'][lag]))],\n",
        "            dtype = precision)\n",
        "        # Disregard if algorithm requires no missing values\n",
        "        if remove_missing :\n",
        "            if (len(np.where(np.isnan(hurricane_x))[0]) > 0) or (len(np.where(np.isnan(hurricane_y))[0]) > 0) :\n",
        "                continue\n",
        "        # Add to our results\n",
        "        x.extend(hurricane_x)\n",
        "        y.extend(hurricane_y)\n",
        "        print(\"Feature engineered {}/{} hurricanes for {} timestep(s)\".format(count, len(hurricanes), timesteps), end = \"\\r\")\n",
        "    print(\"\\nDone feature engineering hurricanes.\")\n",
        "    \n",
        "    return {'x': np.array(x), 'y': np.array(y)}\n",
        "def scaler(processed_data, hurricanes) :\n",
        "    '''\n",
        "    PURPOSE: Scale our data using the RobustScaler method from the sklearn library\n",
        "    METHOD: Generate data using 1 timesteps and then remove the NaN or None types to use the scaler methods\n",
        "    INPUT:  hurricanes - dictionary of hurricane objects\n",
        "            processed_data - dictionary of x and y values of data produced by shape() function with no missing values\n",
        "    OUTPUT: 1) Scaled processed_data using RobustScaler\n",
        "            2) RobustScaler object fit with appropriate data\n",
        "    '''\n",
        "    print(\"Scaling Data . . . (1 timestep for unqiue data)\")\n",
        "    # Create our scaler\n",
        "    unqiue_data = shape(hurricanes, timesteps = 1)\n",
        "    x = np.reshape(unqiue_data['x'], (unqiue_data['x'].shape[0], -1))\n",
        "    x = np.delete(x, np.where(np.isnan(x))[0], 0)\n",
        "    scaler = RobustScaler()\n",
        "    scaler.fit(x)\n",
        "    \n",
        "    # Scale our data\n",
        "    for index in range(len(processed_data['x'])) :\n",
        "        # Scale our x\n",
        "        processed_data['x'][index] = scaler.transform(processed_data['x'][index])\n",
        "        # Scale our y\n",
        "        processed_data['y'][index] = scaler.transform(processed_data['y'][index])\n",
        "    print(\"Done scaling.\")\n",
        "    return processed_data, scaler\n",
        "# Finalize and scale procesed data into a dictionary\n",
        "preprocessed_data = shape(hurricanes, timesteps = 5)\n",
        "processed_data, scaler = scaler(preprocessed_data, hurricanes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NapGhxs0vKF4"
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input, Lambda, Attention, Layer\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.context_vector = self.add_weight(shape=(self.hidden_size,),\n",
        "                                              initializer='random_normal',\n",
        "                                              trainable=True)\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        hidden_states, _ = inputs\n",
        "\n",
        "        context_vector = tf.expand_dims(self.context_vector, axis=0)\n",
        "        context_vector = tf.repeat(context_vector, tf.shape(hidden_states)[0], axis=0)\n",
        "\n",
        "        attention_weights = tf.einsum('ijk,ik->ij', hidden_states, context_vector)\n",
        "        attention_weights = tf.keras.layers.Activation('softmax')(attention_weights)\n",
        "\n",
        "        weighted_sum = tf.einsum('ij,ijk->ik', attention_weights, hidden_states)\n",
        "\n",
        "        return weighted_sum\n",
        "\n"
      ],
      "metadata": {
        "id": "1_LGW5zSQpeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wd7P0bXwvg97"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "def attention_layer(inputs):\n",
        "    hidden_states, context_vector = inputs\n",
        "    \n",
        "    hidden_size = int(hidden_states.shape[2])\n",
        "    \n",
        "    # Reshape context vector to perform element-wise multiplication\n",
        "    context_vector = Dense(hidden_size)(context_vector)\n",
        "    context_vector = tf.repeat(context_vector, tf.shape(hidden_states)[1], axis=1)\n",
        "    \n",
        "    # Attention mechanism\n",
        "    attention_weights = tf.keras.layers.Attention()([hidden_states, context_vector])\n",
        "    attention_weights = tf.keras.layers.Activation('softmax')(attention_weights)\n",
        "    \n",
        "    # Weighted sum of hidden states\n",
        "    weighted_sum = tf.reduce_sum(attention_weights * hidden_states, axis=1)\n",
        "    \n",
        "    return weighted_sum\n",
        "\n",
        "def build_model(input_shape, hidden_units, output_dim, dropout_rate=0.15, recurrent_dropout_rate=0.15):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    lstm_output = LSTM(hidden_units, return_sequences=True, dropout=dropout_rate, recurrent_dropout=recurrent_dropout_rate)(inputs)\n",
        "    attention_output = AttentionLayer(hidden_units)([lstm_output, lstm_output])\n",
        "    time_distributed_output = TimeDistributed(Dense(output_dim))(attention_output)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=time_distributed_output)\n",
        "    return model\n",
        "\n",
        "def train_model(X_train, X_test, y_train, y_test, output_dim, n_epochs):\n",
        "    input_shape = X_train.shape[1:]\n",
        "    hidden_units = 128\n",
        "\n",
        "    model = build_model(input_shape, hidden_units, output_dim)\n",
        "    model.summary()\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "    loss = []\n",
        "    val_loss = []\n",
        "    for epoch in range(n_epochs):\n",
        "        # Early stopping\n",
        "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
        "        history = model.fit(X_train, y_train, epochs=1, batch_size=512, validation_data=(X_test, y_test), verbose=1)\n",
        "        print(\"Loss:\", history.history['loss'][0])\n",
        "        loss.append(history.history['loss'][0])\n",
        "        print(\"Validation Loss:\", history.history['val_loss'][0])\n",
        "        val_loss.append(history.history['val_loss'][0])\n",
        "        print()\n",
        "\n",
        "    return model, history, loss, val_loss\n",
        "\n",
        "# Create our cross-validation data structure\n",
        "# In the first step we will split the data in training and remaining dataset\n",
        "X_train, X_rem, y_train, y_rem = train_test_split(processed_data['x'], processed_data['y'], train_size=0.8)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5)\n",
        "\n",
        "# Define the features to train for\n",
        "features = [2, 0, 1]  # Specify the indices of the features to train for\n",
        "\n",
        "# Train multiple models\n",
        "models = []\n",
        "histories = []\n",
        "losses = []\n",
        "val_losses = []\n",
        "n_epochs = 500\n",
        "\n",
        "for feature_idx in features:\n",
        "    y_train_feature = np.array([[[features[feature_idx]] for features in y] for y in y_train], dtype=np.float64)\n",
        "    y_valid_feature = np.array([[[features[feature_idx]] for features in y] for y in y_valid], dtype=np.float64)\n",
        "    \n",
        "    model, history, loss, val_loss = train_model(X_train, X_valid, y_train_feature, y_valid_feature, output_dim=1, n_epochs=n_epochs)\n",
        "    \n",
        "    models.append(model)\n",
        "    histories.append(history)\n",
        "    losses.append(loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "# Access the history of each model\n",
        "for i, history in enumerate(histories):\n",
        "    print(f\"History for Model {i+1}\")\n",
        "    print(\"Loss:\", history.history['loss'])\n",
        "    print(\"Validation Loss:\", history.history['val_loss'])\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict values\n",
        "wind_predictions = models[0].predict(X_test)\n",
        "lat_predictions = models[1].predict(X_test)\n",
        "long_predictions = models[2].predict(X_test)"
      ],
      "metadata": {
        "id": "7Dl_hKCGKi9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot loss over time\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i, loss in enumerate(losses):\n",
        "    plt.plot(loss, label=f\"Model {i+1}\")\n",
        "plt.title(\"Loss Over Time\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot validation loss over time\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i, history in enumerate(histories):\n",
        "    plt.plot(val_loss, label=f\"Model {i+1}\")\n",
        "plt.title(\"Validation Loss Over Time\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FXrlj3V-MWhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save numpy array as csv file\n",
        "from numpy import asarray\n",
        "from numpy import savetxt\n",
        "# define data\n",
        "wind_history = asarray(history[0])\n",
        "wind_predictions = asarray(wind_predictions)\n",
        "\n",
        "lat_history = asarray(history[1])\n",
        "lat_predictions = asarray(lat_predictions)\n",
        "\n",
        "long_history = asarray(history[2])\n",
        "long_predictions = asarray(long_predictions)\n",
        "\n",
        "# save to csv file\n",
        "savetxt('wind_hist.csv', wind_history, delimiter=',')\n",
        "savetxt('wind_preds.csv', wind_predictions, delimiter=',')\n",
        "savetxt('lat_hist.csv', lat_history, delimiter=',')\n",
        "savetxt('lat_predictions.csv', lat_predictions, delimiter=',')\n",
        "savetxt('long_hist.csv', long_history, delimiter=',')\n",
        "savetxt('long_preds_hist.csv', long_predictions, delimiter=',')"
      ],
      "metadata": {
        "id": "9fYjKBxPXcGp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}